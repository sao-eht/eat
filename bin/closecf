#!/usr/bin/env python

# close ALIST and create HOPS control file steered to global solution
# 2016-10-11 Lindy Blackburn
# 2017-07-20 Lindy Blackburn & CK Chan - update for 2015+ data
# 2017-09-13 Lindy Blackburn, update for 2017 Rev1-Cal

from eat.io import hops, util
import numpy as np
from scipy.optimize import least_squares
from itertools import chain
import pandas as pd
import argparse
import os
import sys
import datetime
import pwd

# logging
statline = '[%s] %s@%s:%s$ %s' % (datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            pwd.getpwuid(os.getuid())[0], os.uname()[1], os.getcwd(), ' '.join(sys.argv))

parser = argparse.ArgumentParser()
parser.add_argument('filename', help='alist txt file')
parser.add_argument('-de', '--delay_error', help='delay error systematic in [us]', type=float, default=2e-6)
parser.add_argument('-re', '--rate_error', help='rate error systematic in [ps/s]', type=float, default=1e-3)
# start with large 2ns delay systematic for delayed leakage, more realistic (2017) on-fringe systematic error is ~20ps
# the large error will effectively drive solution by parallel hands
parser.add_argument('-ce', '--crosspol_delay_error', help='additional systematic for cross pol delay added in quadrature', type=float, default=2e-3)
parser.add_argument('-sw', '--sigmas_to_window', help='sigmas to window for both delay and rate', type=float, default=0)
parser.add_argument('-dw', '--delay_window', help='fixed delay window to apply in [us]', type=float, default=0)
parser.add_argument('-rw', '--rate_window', help='fixed rate window to apply in [ps/s]', type=float, default=0)
args = parser.parse_args()

# read alist file (autodetect version 5 or 6)
a = util.noauto(hops.read_alist(args.filename))
# a = a[a.snr > 9] # loss function not super robust..
# a = a[~a.baseline.str.contains('R')] # SR baseline messes things up, ignore SMA Reference
# a = a[(a.baseline == 'SR') | (~a.baseline.str.contains('R'))] # SR baseline messes things up
# a = a[~(a.baseline == 'SR')] # SR baseline messes things up

dishes = sorted(set(chain(*a.baseline)))

# reverse index of lookup for single dish station code
idish = {f:i for i, f in enumerate(dishes)}

# unwrap mbd to be aligned to sbd
util.unwrap_mbd(a)

# add delay and rate errors, with systematic limits on resolution
util.add_delayerr(a, mbd_systematic=args.delay_error, rate_systematic=args.rate_error,
                     crosspol_systematic=args.crosspol_delay_error)

# extra error for JS baseline, possible noise fringes, SR baseline
a['mbd_err'] = np.sqrt(a['mbd_err']**2 + 50e-6**2*(a.baseline.str.contains('J') & a.baseline.str.contains('S'))
                       + 1.*(a.baseline == 'SR') + 1.*(a.snr < 9.))
a['rate_err'] = np.sqrt(a['rate_err']**2 + 1.*(a.baseline == 'SR') + 1.*(a.snr < 9.))

# least squares objective function: (predict - model) / std
# par is a list of the mdoel parameters fit against alldata and allerr
# idx1: the iloc of the first HOPS station in each baseline
# idx2: the iloc of the second HOPS station in each baseline
def errfunc(par, idx1, idx2, alldata, allerr):
    model = par[idx1] - par[idx2]
    return (alldata - model) / allerr

# indices for unique dishes/dishes
a['idish0'] = [idish[bl[0]] for bl in a.baseline]
a['idish1'] = [idish[bl[1]] for bl in a.baseline]

g = a.groupby('timetag')
scans = sorted(set(a.timetag))

# loop over all scans and overwrite with new solution
# for scan in scans:
for scan in scans:
    idx = g.groups[scan]
    # skip if only one baseline (avoid warning)
    if len(idx) < 2:
        continue
    b = a.loc[idx]
    # initial guess
    rates = np.zeros(len(dishes))
    delays = np.zeros(len(dishes))
    # f_scale will be the deviation [in sigma] where the loss function kicks in
    # it should be a function of the SNR of the detection ideally..
    # but it looks like scipy just supports a single float
    fit_mbd = least_squares(errfunc, np.zeros(len(dishes)),
                            args=(b.idish0, b.idish1, b.mbd_unwrap, b.mbd_err),
                            loss='soft_l1', f_scale=8).x
    fit_rate = least_squares(errfunc, np.zeros(len(dishes)),
                            args=(b.idish0, b.idish1, b.delay_rate, b.rate_err),
                            loss='soft_l1', f_scale=8).x
    a.ix[idx,'mbd_unwrap'] = fit_mbd[b.idish0] - fit_mbd[b.idish1]
    a.ix[idx,'delay_rate'] = fit_rate[b.idish0] - fit_rate[b.idish1]

util.rewrap_mbd(a)

# print out FF control file

print '* ' + statline
g = a.groupby('timetag')
for (scan, b) in g:
    for c in b.groupby('baseline').first().reset_index().itertuples():
        # offsets for ALMA SBD offset
        offset0 = -0.3e-3 if 'A' in c.baseline else 0.
        offset1 = +0.4e-3 if 'A' in c.baseline else 0.
        print("if scan %s and baseline %s sb_win %10.7f %10.7f mb_win %10.7f %10.7f dr_win %15.12f %15.12f * expt_no %d" %
            (c.timetag, c.baseline, c.mbd_unwrap + offset0, c.mbd_unwrap + offset1, c.mbdelay, c.mbdelay, 1e-6*c.delay_rate, 1e-6*c.delay_rate, c.expt_no))


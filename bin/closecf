#!/usr/bin/env python

# close ALIST and create HOPS control file steered to global solution
# 2016-10-11 Lindy Blackburn

from eat.io import hops, util
import numpy as np
from scipy.optimize import least_squares
import pandas as pd
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('filename', help='alist txt file')
args = parser.parse_args()

# stations grouped according to having related fringe parameters
sites = ['DEFG', 'JPQ', 'ST', 'A']
dishes = ['DE', 'FG', 'J', 'PQ', 'ST', 'A']
feeds = [l for l in "DEFGJPQSTA"]
# reverse index of lookup for single dish station code
isite = {f:i for i, flist in enumerate(sites) for f in flist}
idish = {f:i for i, flist in enumerate(dishes) for f in flist}
ifeed = {f:i for i, f in enumerate(feeds)}

# read alist file (autodetect version 5 or 6)
a = hops.read_alist(args.filename)

# unwrap mbd to be aligned to sbd
util.unwrap_mbd(a)

# add delay and rate errors, with systematic limits on resolution
util.add_delayerr(a, mbd_systematic=0.000010, rate_systematic=0.001)

# least squares objective function: (predict - model) / std
# par is a list of the mdoel parameters fit against alldata and allerr
# idx1: the iloc of the first HOPS station in each baseline
# idx2: the iloc of the second HOPS station in each baseline
def errfunc(par, idx1, idx2, alldata, allerr):
    model = par[idx1] - par[idx2]
    return (alldata - model) / allerr

# indices for unique feeds/dishes
a['ifeed0'] = [ifeed[bl[0]] for bl in a.baseline]
a['ifeed1'] = [ifeed[bl[1]] for bl in a.baseline]
a['idish0'] = [idish[bl[0]] for bl in a.baseline]
a['idish1'] = [idish[bl[1]] for bl in a.baseline]

g = a.groupby('scan_id')
scans = sorted(set(a.scan_id))

# loop over all scans and overwrite with new solution
# for scan in scans:
for scan in scans:
    idx = g.groups[scan]
    # skip if only one baseline (avoid warning)
    if len(idx) < 2:
        continue
    b = a.loc[idx]
    # initial guess
    rates = np.zeros(len(dishes))
    delays = np.zeros(len(feeds))
    # f_scale will be the deviation [in sigma] where the loss function kicks in
    # it should be a function of the SNR of the detection ideally..
    # but it looks like scipy just supports a single float
    fit_mbd = least_squares(errfunc, np.zeros(len(feeds)),
                            args=(b.ifeed0, b.ifeed1, b.mbd_unwrap, b.mbd_err),
                            loss='arctan', f_scale=6).x
    fit_rate = least_squares(errfunc, np.zeros(len(dishes)),
                            args=(b.idish0, b.idish1, b.delay_rate, b.rate_err),
                            loss='arctan', f_scale=6).x
    a.ix[idx,'mbd_unwrap'] = fit_mbd[b.ifeed0] - fit_mbd[b.ifeed1]
    a.ix[idx,'delay_rate'] = fit_rate[b.idish0] - fit_rate[b.idish1]

util.rewrap_mbd(a)

# print out FF control file
g = a.groupby('timetag')

for (scan, b) in g:
    for c in b.itertuples():
        print("if scan %s and baseline %s sb_win %8.5f %8.5f mb_win %8.5f %8.5f dr_win %14.11f %14.11f" %
            (c.timetag, c.baseline, c.mbd_unwrap, c.mbd_unwrap, c.mbdelay, c.mbdelay, 1e-6*c.delay_rate, 1e-6*c.delay_rate))

#!/usr/bin/env python
#
# Use ALMA to reference phase-cal phases and delay offsets
# 2017-11-22 Chi-kwan Chan (move from pipeline to EAT)
# 2017-12-28 CKC major update to simplify pipelining

from __future__ import print_function

import datetime, string
import pwd, os, sys
import argparse

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.cm     as cm

import pandas as pd
import numpy  as np

from eat.io   import hops, util
from eat.hops import util as hu

#==============================================================================
# Helper functions

def unwrap(phi):
    """Unwrap an array of angles to avoid artificial jumps due to branching
    """
    for i in np.arange(len(phi)-1)+1:
        if   phi[i] - phi[i-1] >  180: phi[i:] -= 360
        elif phi[i] - phi[i-1] < -180: phi[i:] += 360
    return phi

def offset(phi):
    """Offset an array of angles (in degree) to the principle branch
    """
    while True:
        m = np.mean(phi)
        if   m >  180: phi -= 360
        elif m < -180: phi += 360
        else         : break
    return phi

def load(file):
    """Read an alist file and prepare phases phase-cal

    Read an alist file, apply filter to select the userful rows, and
    then access the corel (per-channel) phases for performing
    phase-cal.
    """
    a = util.noauto(hops.read_alist(file))
    if not args.no_fix:
        util.fix(a) # fix various polconvert errors

    # Apply row filtering
    a = a[a.snr > 50]
    a = a[a.baseline.str.contains("A")] # must contain ALMA
    a = a[(a.polarization == 'LL')|
          (a.polarization == 'RR')|
          (a.polarization == 'XR')|
          (a.polarization == 'XL')|
          (a.polarization == 'YR')|
          (a.polarization == 'YL')]
    a = a.reset_index(drop=True)
    util.add_path(a)

    # Access corel (per-channel) visibility
    dflist = []
    for i, f in a.path.items():
       ff_df = hu.params(f)

       # perform quick adhoc phase correction and obtain a single averaged value
       v = hu.adhoc(f, roundrobin=False).vcorr.mean(axis=0)
       p = np.angle(np.mean(v))

       # compute vis
       vis = v * np.exp(-1j * p)

       # if ALMA is not the reference, reverse baseline label and negate vis
       b = a.baseline[i]
       if b[1] == "A":
           a.loc[i, 'baseline'] = b[::-1]
           vis = np.conj(vis)
           print("WARNING: ALMA is not the reference", file=sys.stderr)

       # construct temp dataframe
       tempdf = pd.DataFrame(vis[np.newaxis,:], columns=ff_df.fedge)
       dflist.append(tempdf)

    df = pd.concat(dflist)
    df = df.reset_index(drop=True)
    df = df.sort_index(axis=1)
    df.columns = list(string.ascii_letters[:df.columns.shape[0]]) # assign ASCII channel codes to frequencies

    # add metadata from a to df
    cols  = ['polarization', 'baseline', 'quality', 'snr', 'resid_phas']
    return pd.concat([a[cols], df], axis=1)


def qfit(y1, y2, y3):
    """Quadratic fits
    """
    a  =       y1
    b  = (-3.0*y1 + 4.0*y2 - y3) / 2.0
    c  = (     y1 - 2.0*y2 + y3) / 2.0
    return a, b, c

def fix_nan(y):
    """Extrapolation for last channel
    """
    a, b, c = qfit(y[:,-4], y[:,-3], y[:,-2])
    last    = a + 3.0*b + 9.0*c # == a + b x + c x**2 at x == 3
    m       = np.isnan(y[:,-1])
    y[m,-1] = last[m]
    return y

def derivative(y, dx=1.0):
    """Compute derivative of y
    """
    a, b, c = qfit(y[:,-3], y[:,-2], y[:,-1])
    m1 = b + 4.0*c # == b + 2 c x with x == 2
    a, b, c = qfit(y[:,0], y[:,1], y[:,2])
    m0 = b         # == b + 2 c x with x == 0
    return np.concatenate((np.reshape(m0, (-1,1)),
                           0.5 * (y[:,2:] - y[:,:-2]),
                           np.reshape(m1, (-1,1))),
                          axis=1) / dx

def mkcf(baselines, code, data):
    sites = [b[1] for b in baselines]
    chans = string.ascii_letters[:data.shape[1]]
    return ["""if station {}
  {} {}
    {}
""".format(s, code, chans, " ".join(["{:.6f}".format(p) for p in -data[i,:]]))
            for i, s in enumerate(sites)]

def mkcsv(baselines, data, file):
    out = pd.DataFrame(data)
    out.insert(0, 'label', baselines)
    out.to_csv(file, index=False, header=False, sep=" ", float_format="%.9g")

def mkplot(baselines, phases, slopes, file):
    cs = cm.rainbow(np.linspace(0,1,len(baselines)))
    d  = np.array([-0.5,0.5])
    x  = np.arange(phases.shape[1])
    plt.figure()
    for i, b in enumerate(baselines):
        y = -phases[i,:]
        m = -slopes[i,:]
        plt.scatter(x, y, color=cs[i], label=b)
        for j in x:
            plt.plot(x[j]+d, y[j]+d*m[j], color=cs[i])
    plt.legend()
    if "." in file:
        plt.savefig(file)
    else:
        plt.show()

#==============================================================================
# Main code
#------------------------------------------------------------------------------
# Logging
statline = "[{}] {}@{}:{}$ {}".format(
    datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    pwd.getpwuid(os.getuid())[0], os.uname()[1], os.getcwd(),
    ' '.join(sys.argv))

#------------------------------------------------------------------------------
# Argument parsing
try:
    datadir = os.environ['DATADIR']
except:
    datadir = None

parser = argparse.ArgumentParser(
    description="Estimate phase-cal phases and delay offsets")
parser.add_argument("filename",
                    help="alist file")
parser.add_argument("-d", "--datadir", default=datadir,
                    help="use fringe files from this data directory")
parser.add_argument("-a", "--average", default=False, action="store_true",
                    help="use average L and R phase cal")
parser.add_argument("-o", "--outfile", default=[sys.stdout], nargs="+",
                    help="output file name(s); "+
                         "if \"-c\" is used, only 1 file name is allowed; "+
                         "otherwise, at most 5 names can be used")
parser.add_argument("-c", "--controlcodes", default=False, action="store_true",
                    help="construct control codes rather than csv table")
parser.add_argument("-g", "--graph",        default=False, action="store_true",
                    help="graphic output")
parser.add_argument('-nf', '--no-fix', help='do not apply alist fixes', action="store_true")
args = parser.parse_args()

if len(args.outfile) > 5:
    print("At most 4 output files are supported")
    exit(1)
elif args.controlcodes and len(args.outfile) != 1:
    print("Only 1 output file is supported when \"--controlcodes\" is set")
    exit(1)

#------------------------------------------------------------------------------
# Get data, perform phase cal, and compute delay offsets
if args.datadir is not None:
    hu.set_datadir(args.datadir)

df = load(args.filename)
rr = df[df.polarization.str[1] == 'R'] # support mixed products: RR, XR, YR
ll = df[df.polarization.str[1] == 'L']

nchan = string.ascii_letters.find(df.columns[-1]) + 1
chans = list(string.ascii_letters[:nchan])

# at some point, pandas.core.groupby.GroupBy.mean() no longer considers numpy.complex128 to be numeric type (?!!)
# update: previous issue may be related to change in numeric_only default handling in 1.3.0
# https://github.com/pandas-dev/pandas/issues/42395
# however as of 1.3.4 groupby aggregation functions are now completely broken for complex values
# including simple aggregation such as "first()"
# https://github.com/pandas-dev/pandas/issues/43701
# here we could replace "mean" with "sum" -- which for some reason still works
# but since groupby operations on complex values are so common, may want to simply revert pandas
pcf   = df.groupby('baseline')[chans].sum(numeric_only=False).apply(np.angle, deg=True)
pcr   = rr.groupby('baseline')[chans].sum(numeric_only=False).apply(np.angle, deg=True)
pcl   = ll.groupby('baseline')[chans].sum(numeric_only=False).apply(np.angle, deg=True)

pcf[:] = fix_nan(pcf.apply(unwrap, axis=1).apply(offset, axis=1).values)
pcr[:] = fix_nan(pcr.apply(unwrap, axis=1).apply(offset, axis=1).values)
pcl[:] = fix_nan(pcl.apply(unwrap, axis=1).apply(offset, axis=1).values)

slopes = derivative(pcf.values)
d_nu   = 58.59375e6 # hard wire separation between channels
delays = 1e9 * (slopes / 360.0) / d_nu # in nanosecond

#------------------------------------------------------------------------------
# Outputs
if args.controlcodes:
    need_open_close = not hasattr(args.outfile[0], "write")
    if need_open_close:
        f = open(args.outfile[0], "w")
    else:
        f = args.outfile[0]
    f.write("* {}\n\n".format(statline))
    if args.average:
        f.write("\n".join(mkcf(pcr.index.values, "pc_phases", pcf.values) +
                          mkcf(pcf.index.values, "delay_offs",  delays)))
    else:
        f.write("\n".join(mkcf(pcr.index.values, "pc_phases_r", pcr.values) +
                          mkcf(pcl.index.values, "pc_phases_l", pcl.values) +
                          mkcf(pcf.index.values, "delay_offs",  delays)))
    if need_open_close:
        f.close()
else:
    k = list("rlfds")
    b = {'r': pcr.index.values,
         'l': pcl.index.values,
         'f': pcf.index.values,
         'd': pcf.index.values,
         's': pcf.index.values}
    v = {'r': pcr.values,
         'l': pcl.values,
         'f': pcf.values,
         'd': delays,
         's': slopes}
    if len(args.outfile) == 1:
        f = args.outfile[0]
        if hasattr(f, "write"):
            for j in k:
                mkcsv(b[j], v[j], f)
        else:
            p, x = os.path.splitext(f)
            for j in k:
                mkcsv(b[j], v[j], p+'_'+j+x)
    else:
        for i, f in enumerate(args.outfile):
            mkcsv(b[k[i]], d[k[i]], f)

if args.graph:
    k = pcf.index.values
    mkplot(pcf.index.values, pcf.values, slopes,      "pc_phase_f.pdf")

    i = [np.where(k == b)[0][0] for b in pcr.index.values]
    mkplot(pcr.index.values, pcr.values, slopes[i,:], "pc_phase_r.pdf")

    i = [np.where(k == b)[0][0] for b in pcl.index.values]
    mkplot(pcl.index.values, pcl.values, slopes[i,:], "pc_phase_l.pdf")
